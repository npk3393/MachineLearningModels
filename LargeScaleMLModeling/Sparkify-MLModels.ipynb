{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import * \n",
    "# isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import * \n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import *\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# run the session and prepare the data\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CapstoneProject\") \\\n",
    "    .getOrCreate()\n",
    "path = \"mini_sparkify_event_data.json\"\n",
    "sparkify = spark.read.json(path)\n",
    "df_valid = sparkify.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "df_valid = df_valid.filter(df_valid[\"userId\"] != \"\")\n",
    "df_valid = df_valid.sort([\"userId\",\"ts\",\"sessionId\"]).withColumn(\"pk\",monotonically_increasing_id())\n",
    "sparkify.createOrReplaceTempView(\"sparkifytable\")\n",
    "churnUserId = df_valid.filter(df_valid.page == \"Cancellation Confirmation\").select('UserId').distinct().toPandas()\n",
    "churnUserId = churnUserId.UserId.tolist()\n",
    "df_valid = df_valid.withColumn('ChurnedUser', when(df_valid.userId.isin(churnUserId),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Model Implementation \n",
    "From our EDA, we would like to consider, length of songs played, average length of played songs, number of days active for all subscribers during free and paid levels of subscription, total number of sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering and data prep for modeling\n",
    "# total length of the song\n",
    "songlength = df_valid.where(df_valid.song != 'null').groupby('userId') \\\n",
    ".agg({'length':'sum'}) \\\n",
    ".orderBy('userId') \\\n",
    ".withColumnRenamed(\"sum(length)\", \"songlength\")\n",
    "# average length of the song\n",
    "songlengthavg = df_valid.where(df_valid.song != 'null').groupby('userId') \\\n",
    ".agg({'length':'avg'}) \\\n",
    ".orderBy('userId') \\\n",
    ".withColumnRenamed(\"avg(length)\", \"songlengthavg\")\n",
    "# number of songs\n",
    "songcount = df_valid.where(df_valid.song != 'null').groupby('userId') \\\n",
    ".agg({'song':'count'}) \\\n",
    ".orderBy('userId') \\\n",
    ".withColumnRenamed(\"count(song)\", \"songcount\")\n",
    "# number of sessions\n",
    "numsessions = df_valid.select(['userId','sessionId']) \\\n",
    ".distinct() \\\n",
    ".groupby('userId') \\\n",
    ".count() \\\n",
    ".orderBy('userId') \\\n",
    ".withColumnRenamed('count','numsessions')\n",
    "# Add friend event\n",
    "friends = df_valid.where(df_valid.page == \"Add Friend\") \\\n",
    ".groupby('userId').agg({'page':'count'})\\\n",
    ".orderBy('userId')\\\n",
    ".withColumnRenamed(\"count(page)\",\"friendcount\")\n",
    "friends = friends.replace(float('nan'), None)\n",
    "# Add Playlist event\n",
    "playlist = df_valid.where(df_valid.page == \"Add to Playlist\") \\\n",
    ".groupby('userId').agg({'page':'count'})\\\n",
    ".orderBy('userId')\\\n",
    ".withColumnRenamed(\"count(page)\",\"addtoplaylist\")\n",
    "# churn data\n",
    "label = df_valid.groupby('userId').agg({'ChurnedUser':'first'}).orderBy('userId')\\\n",
    ".withColumnRenamed('first(ChurnedUser)','label')\n",
    "# number of days active for users\n",
    "time = 60*60*24*1000\n",
    "min_ts = df_valid.select(['userId','ts']).groupby('userId').min('ts')\n",
    "max_ts = df_valid.select(['userId','ts']).groupby('userId').max('ts')\n",
    "days_active = min_ts.join(max_ts,on='userId')\n",
    "days_active = days_active.withColumn('days_active',(col('max(ts)')-col('min(ts)'))/time)\n",
    "days_active = days_active.select(['userId','days_active']).orderBy('userId')\n",
    "modelfile = (songcount\n",
    "             .join(days_active, 'userId','full')\n",
    "             .join(friends, 'userId', 'full')\n",
    "             .join(numsessions, 'userId','full')\n",
    "             .join(playlist, 'userId','full')\n",
    "             .join(label,'userId','full'))\n",
    "modelfile = modelfile.fillna({'friendcount':0})\n",
    "modelfile = modelfile.fillna({'addtoplaylist':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = modelfile.randomSplit([0.8,0.2])\n",
    "features = train.drop('label','userId').schema.names\n",
    "vector_assembler = VectorAssembler(inputCols=features,outputCol = 'Features')\n",
    "modelinput = vector_assembler.transform(modelfile)\n",
    "Scaler1 = StandardScaler(withMean=True,withStd=True,inputCol='Features',outputCol='ScaledFeatures')\n",
    "FeatureScaler1Fit = Scaler1.fit(modelinput)\n",
    "ScaledInput1 = FeatureScaler1Fit.transform(modelinput)\n",
    "modeldata = ScaledInput1.select(ScaledInput1.label.alias('label'),ScaledInput1.ScaledFeatures.alias('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = modeldata.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Logit\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "f1_evaluator=MulticlassClassificationEvaluator(metricName='f1')\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "crossvalidation_baselr = CrossValidator(estimator=lr,\n",
    "                                        evaluator=f1_evaluator,\n",
    "                                        estimatorParamMaps=paramGrid,\n",
    "                                        numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7862241999886661]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselr = crossvalidation_baselr.fit(train)\n",
    "baselr.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7846153846153846\n",
      "F1 Score:0.7659763313609467\n"
     ]
    }
   ],
   "source": [
    "baselrtest = baselr.transform(test)\n",
    "evaluator=MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print('Accuracy:{}'.format(evaluator.evaluate(baselrtest,{evaluator.metricName: \"accuracy\"})))\n",
    "print('F1 Score:{}'.format(evaluator.evaluate(baselrtest,{evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "f1_evaluator=MulticlassClassificationEvaluator(metricName='f1')\n",
    "dtc_parameterGrid = ParamGridBuilder().build()\n",
    "dtc_crossval = CrossValidator(estimator = dtc, estimatorParamMaps=dtc_parameterGrid, \n",
    "                             evaluator=f1_evaluator,\n",
    "                             numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.765654048348515]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basedtc = dtc_crossval.fit(train)\n",
    "basedtc.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8307692307692308\n",
      "F1 Score:0.8324610097805973\n"
     ]
    }
   ],
   "source": [
    "basedtc = dtc_crossval.fit(train)\n",
    "\n",
    "\n",
    "basedtctest = basedtc.transform(test)\n",
    "evaluator=MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print('Accuracy:{}'.format(evaluator.evaluate(basedtctest,{evaluator.metricName: \"accuracy\"})))\n",
    "print('F1 Score:{}'.format(evaluator.evaluate(basedtctest,{evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling Methodology\n",
    "# splitting the data into training test and testing set \n",
    "\n",
    "# vectorize the features by dropping the variables not needed for prediction - outcome variable and userID as well\n",
    "\n",
    "# we are scaling the features to overcome any variable that has a larger scale overpowering the model\n",
    "#scale = MinMaxScaler(inputCol='Features',outputCol='Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# Pipelines took really long time\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline_lr = Pipeline(stages=[lr])\n",
    "f1_evaluator=MulticlassClassificationEvaluator(metricName='f1')\n",
    "\n",
    "\n",
    "# create parameter grids\n",
    "lr_parameterGrid = (ParamGridBuilder().addGrid(lr.regParam,[0.0,0.1]).build())\n",
    "\n",
    "# cross validation 3-step\n",
    "lr_crossval = CrossValidator(estimator=pipeline_lr, \n",
    "                            estimatorParamMaps=lr_parameterGrid,\n",
    "                            evaluator=f1_evaluator,\n",
    "                            numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "pipeline_dtc = Pipeline(stages=[dtc])\n",
    "\n",
    "# creating parameter grids\n",
    "dtc_parameterGrid = ParamGridBuilder().addGrid(dtc.maxDepth,[1,2]).build()\n",
    "\n",
    "# cross validation 3-step\n",
    "dtc_crossval = CrossValidator(estimator = pipeline_dtc, \n",
    "                              estimatorParamMaps=dtc_parameterGrid, \n",
    "                             evaluator=f1_evaluator,\n",
    "                             numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Keeping our machine learning analysis modest, we are running logistic regression and decision tree classifier on our sparkify subscribers data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Py4jError troubleshooting - do not use if the model trains successfully\n",
    "spark.sparkContext.setCheckpointDir('checkpoint')\n",
    "train.checkpoint()\n",
    "#train.explain(extended=True)\n",
    "train = spark.createDataFrame(train.rdd, schema=train.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trained_lr = lr_crossval.fit(train)\n",
    "dtc_train = dtc_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_460c912c35ef, numClasses = 2, numFeatures = 5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the hyperpar\n",
    "bestModellr = trained_lr.bestModel\n",
    "#print(bestModellr.getRegParam())\n",
    "#print(bestModellr.getMaxIter())\n",
    "bestModellr.stages[-1]\n",
    "#bestModellr._java_obj.getRegParam()\n",
    "bestModellr.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bestModeldtc = dtc_train.bestModel\n",
    "dtc_train1 = dtc_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_eed9dd4a1784) of depth 2 with 5 nodes"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_train.bestModel.stages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation and Metrics Evaluation\n",
    "\n",
    "We will be looking at accuracy metric and f1-score for both these algorithms to evaluate their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8642026825633384\n",
      "Accuracy: 0.8688524590163934\n",
      "f1 score: 0.8642026825633384\n"
     ]
    }
   ],
   "source": [
    "# validation logistic regresssion\n",
    "evaluator=MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "logisticregression = trained_lr.transform(test)\n",
    "lr_score = evaluator.evaluate(logisticregression)\n",
    "print(lr_score)\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(logisticregression, {evaluator.metricName: 'accuracy'})))\n",
    "print('f1 score: {}'.format(evaluator.evaluate(logisticregression, {evaluator.metricName: 'f1'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8032786885245902\n",
      "f1 score: 0.7761664564943254\n"
     ]
    }
   ],
   "source": [
    "# validation decision tree classifier\n",
    "\n",
    "decisiontree = dtc_train.transform(test)\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(decisiontree, {evaluator.metricName: 'accuracy'})))\n",
    "print('f1 score: {}'.format(evaluator.evaluate(decisiontree, {evaluator.metricName: 'f1'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = [0.7846153846153846, 0.7659763313609467]\n",
    "dt1 = [0.8307692307692308, 0.8324610097805973]\n",
    "lr2 = [0.8688524590163934, 0.8642026825633384]\n",
    "dt2 = [0.8032786885245902, 0.7761664564943254]\n",
    "hz = ['Logit', 'DecisionTree']\n",
    "vt = ['BeforeTuning', 'AfterTuning']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Justification \n",
    "\n",
    "The logistic regression yields an accuracy score of ... and f1 score of ..., for our churning model.\n",
    "\n",
    "The decision tree classifier yields an accuracy score of 0.7391304347826086 and f1 score of 0.7171954563258911. This metric is quite impressive as having developed predictive models in my previous assignments with real world data with variations within data set that is out of control, any accuracy score above 60% is considered a good metric to me.\n",
    "\n",
    "Lack of sample size is concerning and potentially effectiveness will be more obvious if the number of users were to be in few thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection and Improvement\n",
    "\n",
    "As someone who has played with Apache Spark during Scala days, it was a pleasure to discover the PySpark matured into a proper package with full support for data frames and growing traction for PySpark ML library. \n",
    "\n",
    "Also, at the same time, I understand the pain points of running relatively small batch of data into a distributed system. It is going to be ridicule to use RDD for a data set that has little over 250 rows with few columns. Nevertheless, it has been a great practice for me and I remain thanking for that.\n",
    "\n",
    "The data set is somewhat cleaned and relatively less messy. The behavior of churners (subscribers) does not seem relatively trivial -- one would imagine someone spends less time on the streaming service, does not add more friends or songs to play list would likely to dropout however we have good mix of data that has subscribers who appear to be loyal to service and yet have decided to downgrade or execute the event \"cancellation confirmation\" so this was interesting.\n",
    "\n",
    "There is also the concept of system tuning or troubleshooting the resource allocation (job scheduling, memory reallocation, etc...) from the system level that would play a crucial role in duration it takes for each job to run and data to persist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
